# 机器学习(Machine Learning)
- 监督学习(Supervised Learning)：训练样本带有信息标记(**y值**)，利用已有的训练样本信息学习数据的规律预测未知的新样本标签。
  - 回归分析(Regression)
  - 分类(Classification)
- 无监督学习(Unsupervised Learning)：训练样本的标记信息时未知的，目测是为了揭露训练样本的内在数学，结构和信息，为进一步的数据挖掘提供基础。
  - 聚类(Clustering)
## 1 回归
回归是研究自变量x对因变量y影响的一种数据分析方法。主要应用场景是**进行预测和空值**，例如，计划制定、KPI制定、目标制定等；也可以基于预测的数据与实际数据进行比对和分析，确定事件发展程度并给未来行动提供方向性指导。

回归分析可应用于分析自变量和因变量的影响关系(已知x，求y),也可以分析自变量对因变量的影响方向(正向or反向影响)。

**常用的回归算法包括**：
- 线性回归
- 二项式回归
- 对数回归
- 指数回归
- 核SVM
- 岭回归
- Lasso

优点：
- 数据模式和结果便于理解，如线性回归用y=ax+b的形式表达
- 基于函数公式的业务应用中，可直接代入法求解，应用起来容易。

缺点：
- 只能分析少量变量间相互关系，无法处理海量变量间的相互作用关系，尤其是变量共同因素对因变量的影响程度。

### 1.1 注意回归变量之间的共线性问题
检验共线性的三个指标：
- 容忍度：[0,1]，每个自变量(x)作为因变量(y)进行回归建模得到的残差比例。值越小，说明共线性问题的可能性越大。
- 方差膨胀因子：容忍度的倒数，值越大则共线性问题越明显，通常以10作为判断边界。VIF<10,不存在多重共线性；10<=VIF<=100,存在较强的多重共线性；VIF>=100,存在严重多重共线性
- 特征值：对自变量进行主成分分析，如果多个维度的特征值等于0，则可能存在比较严重的共线性。
- 相关系数：R>0.8：可能存在较强的相关性。

**解决共线性的5种常用方法**：
- 增大样本量
- 岭回归法
- 逐步回归法
- 主成分回归
- 人工去除

### 1.2 相关系数、判定系数和回归系数之间的关系
假设一回归方程：y = 42.738x + 169.94，其中R的平方 = 0.5252，如果对这两个变量作相关性分析，还会得到相关系数R=0.72468551874050.

回归系数：42.738，自变量x的**回归系数**；0.5252是该方程的**判定系数**；0.724....是两个变量的**相关性系数**。
- 判定系数：自变量对因变量的方差解释程度的值；计算公式为：回归平方和与总离差平方和之比值
- 相关系数：又称为解释系数，是衡量变量间的相关程度或密切程度的值，本质是线性相关性的判断。

三者间的关系：
- 判定系数是**所有参与模型中自变量的对因变量联合影响程度**，而非某个自变量的影响程度。
- 回归系数与相关系数的关系：回归系数>0,相关系数取值为(0,1)。说明两者正相关；如果系数小于0，相关系数取值为(-1,0)，说明两者负相关。

## 2 分类算法
- 一种对**离散型随机变量**建模或预测的监督学习算法。
- 使用案例包括邮件过滤、金融欺诈和预测雇员异动等输出为类别的任务。
- 分类算法通常适用于预测一个类别(或类别的概率)而不是连续的数值。

### 2.1 分类算法的应用
- 预测
- 提炼应用规则
- 提取变量特征
- 处理缺失值

### 2.2 如何选择分类分析算法
- 文本分类： **朴素贝叶斯**，如电子邮件中的垃圾邮件识别。
- 若训练集较小，选择高偏差且低方差的分类算法效果更好，如**朴素贝叶斯、支持向量机，因为这类算法不容易过拟合**。
- 如果关注的是算法模型的计算时间和模型易用性，那么支持向量机、人工神经网络不是好的选择。
- 如果重视算法的准确率，应选择精度较高的方法，如**支持向量机或GBDT、XGBOOST等基于Boosting的集成方法**。
- 如果注重效果的稳定性或模型鲁棒性，那么应选择**随机森林、组合投票模型等基于Bagging的集成方法**。
- 如果想得到有关预测结果的概率信息，然后**基于预测概率做进一步的应用，那么使用逻辑回归是比较好的选择**。
- 如果**担心离群点或数据不可分并且需要清晰的决策规则，那么选择决策树**。

### 2.3 (基础)决策树 Decision Tree
决策树是一个树结构(可以是二叉树或非二叉树)。

其每个非叶节点表示一个**特征属性**上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。

使用决策树进行决策的过程就是从**根节点开始**,测试待分类项中相应的特征属性，并按照其值选择输出分支，知道到达叶子节点，将**叶子节点**存放的类别作为决策结果。

**优点：**
- 适用任何类型的数据(类别变量更普遍)
- 直观、决策树可以提供可视化，便于理解
- 模型预测出的结果简单，可解释性强
- 适用于小规模数据

**缺点：**
- 当数据中存在连续变量的属性时，决策树表现并不是很好
- 不稳定性，一点点的扰动或者改动都可能改动整棵树
- 特殊属性增加时，错误增加的比较快
- 很容易在训练数据中生成复杂的树结构，造成过拟合。

### 2.4 随机森林 Random Forest
![image](https://github.com/teamowu/Machine-Learning/blob/master/images/Random%20Forest.png)

**优点：**
- 随机森林不容易限于过拟合
- 具有很好的抗噪声能力
- 处理很高维度(feature多)的数据，并且不用做特征选择
- 训练速度快

## 3 聚类
- 一种无监督式机器学习(即**数据没有标注**)
- 算法基于数据的内部结构寻找观察样本的自然族群(即集群)
- 使用案例包括客户细分，新闻聚类，文章推荐等等。

**用于衡量相似性的几个指标**：
- 欧式距离 Euclidean distance
  - 定义：指在m维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离）
  - 用途：
  
![image](https://github.com/teamowu/Machine-Learning/blob/master/images/%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB.png)

- 曼哈顿距离 Manhattan distance
  - 定义：就是表示两个点在标准坐标系上的绝对轴距之和。
  - 用途：
  
![image](https://github.com/teamowu/Machine-Learning/blob/master/images/%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB.png)

- 余弦相似性 cosine
  - 定义：通过计算两个向量的夹角余弦值来评估他们的相似度。
  - 用途：新闻分类
  
![image](https://github.com/teamowu/Machine-Learning/blob/master/images/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E6%80%A7.png)

- Jaccard系数
  - 定义：给定两个集合A,B，Jaccard 系数定义为A与B交集的大小与A与B并集的大小的比值。
  - 用途：用于比较有限样本集之间的相似性与差异性。Jaccard系数值越大，样本相似度越高。
  
![image](https://github.com/teamowu/Machine-Learning/blob/master/images/jaccard%E7%B3%BB%E6%95%B0.png)

### 3.1 层次聚类 Hierarchical Cluster Analysis(HCA)
层次聚类是一系列基于以下概念的聚类算法：
- 最开始由一个数据点作为一个集群
- 对于每个集群，基于相同的标准合并集群
- 重复这一过程直到只留下一个集群，因此就得到了集群的层次结构。

### 3.2 K均值聚类 K-means Clustering Algorithm
- 聚类的度量基于样本点之间的几何距离(即在坐标平面中的距离)
- 集群是围绕在聚类中心的族群，而集群呈现出类球状并具有相似的大小
- 对于给定的k值，算法先给出一个初始的分组方法，然后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案较前一次好

### 3.3 DBSCAN
- 基于密度的算法，它将样本点的密度区域组成一个集群
- DBSCAN不需要假设集群为球状，并且它的性能是可拓展的
- 不需要每个点都被分配到一个集群中，这降低了集群的异常数据。
  

