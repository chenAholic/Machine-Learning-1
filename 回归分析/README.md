# 回归分析
如何选择回归分析算法？
- 简单线性回归。适合数据集本身结构简单、分布规律有明显线性关系的场景。
- 自变量数量少或降维后得到了可以使用的二维变量(包括预测变量)可以直接通过散点图发现自变量和因变量的相互关系，然后选择最佳回归方法。
- 如果经过基本判断发现自变量间有较强的共线性关系，那么可以使用对多重共线性(自变量高度相关)能灵活处理的算法，例如岭回归。
- 如果**数据集噪音较多，推荐使用主成分回归**，因为各主成分回归通过对参与回归的主成分的合理选择，可以去掉噪音。另外，对各个主成分间相互正交，能解决多元线性回归中的
共线性问题。这些都能有效地提高模型的抗干扰能力。
- 如果高维度变量下，使用正则化回归方法效果最好，例如Lasso,Ridge和ElasticNet;或者使用逐步回归从中挑选出影响显著的自变量来建立回归模型。
- 如果要同时验证多个算法，并想从中选择一个来做好的拟合，可以使用交叉检验做多个模型的效果对比，并通过R-square，Adjusted R-squre,AIC,BIC以及各种残差、误差
项指标做综合评估。
- 如果注重模型的可解释性，那么容易理解的线性回归，指数回归，对数回归，二项或多项式回归要比核回归，支持向量机等更适合。
- 集成或组合回归方法。一旦确认了几个方法，但又不确定该如何取舍，可以将多个回归模型做集成或组合方法使用，即同时将多个模型的结果通过加权、均值等方式确定最终输出结果值。
